{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e72e9-aed3-4d46-955f-ceae279bd5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11dc8abe-329b-47c1-8e57-487b34c665f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04f3786-8306-4963-9b61-d3d10f8acd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_policy_files(folder_path):\n",
    "    \"\"\"\n",
    "    Renames PDF files in the specified folder by removing \"LIC's\" from their filenames.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDF files.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\") and \"LIC's\" in filename:\n",
    "            new_filename = filename.replace(\"LIC's\", \"\").strip()\n",
    "            old_path = os.path.join(folder_path, filename)\n",
    "            new_path = os.path.join(folder_path, new_filename)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f'Renamed: {filename} -> {new_filename}')\n",
    "\n",
    "rename_policy_files(\"../../policy_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4d290-50c7-45d6-a84c-cb939cfa169d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1991f57-a969-4c3b-9992-85747e2cb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_from_pdfs(folder_path, output_folder=\"processed_chunks\"):\n",
    "    \"\"\"\n",
    "    Extracts chunks from all PDFs in the folder, saves them as JSON files, and preserves `orig_elements`.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDF files.\n",
    "        output_folder (str): Path to save processed chunks.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "    \n",
    "    for file in tqdm(pdf_files, desc=\"Processing PDFs\", unit=\"file\", leave = False):\n",
    "        output_file = os.path.join(output_folder, f\"{file}.json\")\n",
    "        \n",
    "        # Skip processing if the file is already saved\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {file}, already processed.\")\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        chunks = partition_pdf(\n",
    "            filename=file_path,\n",
    "            infer_table_structure=True,\n",
    "            strategy=\"hi_res\",\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=10000,\n",
    "            combine_text_under_n_chars=2000,\n",
    "            new_after_n_chars=6000,\n",
    "        )\n",
    "        \n",
    "        # Convert chunks to JSON serializable format\n",
    "        chunk_data = []\n",
    "        for chunk in chunks:\n",
    "            chunk_dict = chunk.to_dict()\n",
    "            \n",
    "            # Preserve original elements if available\n",
    "            if hasattr(chunk.metadata, \"orig_elements\") and chunk.metadata.orig_elements:\n",
    "                chunk_dict[\"metadata\"][\"orig_elements\"] = [elem.to_dict() for elem in chunk.metadata.orig_elements]\n",
    "            \n",
    "            chunk_data.append(chunk_dict)\n",
    "        \n",
    "        # Save to JSON file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunk_data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Processed and saved {file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d331e5-d9c7-465e-ab31-51bcce16b57d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|                                             | 0/77 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping New Children's Money Back Plan - Sales Brochure .pdf, already processed.\n",
      "Skipping Single Premium Endowment Plan - Sales Brochure .pdf, already processed.\n",
      "Skipping New Money Back Plan- 20 Years - CIS  New Money Back Plan-20 years .pdf, already processed.\n",
      "Skipping New Endowment Plan - Sales brochure .pdf, already processed.\n"
     ]
    }
   ],
   "source": [
    "all_chunks = extract_chunks_from_pdfs(\"../../policy_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1ff40-bf58-43e5-86aa-9816667d82e0",
   "metadata": {},
   "source": [
    "1. Extract tables from each json file's metadata.orig_elements separately.\n",
    "2. Extract text simply from the chunks json text\n",
    "3. Summarise the text_list and the tables_list\n",
    "4. Create the database where both parent_chunks and the summaries have the same ID\n",
    "5. we query the embeddings of the summaries and retrieve the original docs\n",
    "6. Original docs are then used to give the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4f5bf-db05-4167-bfe5-7456bb45e597",
   "metadata": {},
   "source": [
    "BART-large-cnn for summarising the chunks\n",
    "Roberta base from huggingface for embeddings\n",
    "langchain chroma as vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0bc64d1-910e-4ace-89a7-7c3148205ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590780c-e344-4622-bcbb-769039dd9835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
